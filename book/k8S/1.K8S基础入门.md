# 1.Kubernetes 基础入门

# Kubernetes 基础入门

## K8S 参考网站

官网：[TP](https://kubernetes.io/)

kubeadm 官方文档：[TP](https://kubernetes.io/zh/docs/setup/production-environment/tools/)

docker 官方文档：[TP](https://docs.docker.com/)

prometheus 官方文档：[TP](https://prometheus.io/docs/introduction/overview/)

ansible 安装 k8s 项目：[TP](https://github.com/easzlab/kubeasz)

阿里云 ACK：[TP](https://cn.aliyun.com/product/kubernetes)

亚马逊云 EKS：[TP](https://aws.amazon.com/cn/eks/)

## K8S 介绍

kubeadm 官方介绍：[TP](https://kubernetes.io/zh-cn/docs/concepts/overview/)

此页面是 Kubernetes 的概述。

Kubernetes 是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。

**Kubernetes** 这个名字源于希腊语，意为 “舵手” 或 “飞行员”。k8s 这个缩写是因为 k 和 s 之间有八个字符的关系。 Google 在 2014 年开源了 Kubernetes 项目。 Kubernetes 建立在 [Google 大规模运行生产工作负载十几年经验](https://research.google/pubs/pub43438)的基础上， 结合了社区中最优秀的想法和实践。



![image-20230918155745061](https://xiaoyuanwiki.com/image/image-20230918155745061.png)



```bash
borg
swarm
mesos
 
声明式管理
```

## K8S 组件

### master (控制平面组件 Control Plane Components)

```bash
kube-apiserver：司令部（所有组件之间沟通，都需要经过apiserver）
etcd：存储K8S所有数据的数据库
kube-scheduler：资源计算、资源调度
kube-controller：控制器
```

### node

```bash
kubelet：启动容器，创建pod
container-runtime：容器运行时
kube-proxy：网络、端口映射
```

**例子**：要创建 10 个 nginx 就需要编写一份资源清单去交给 `apiserver`,`apiserver` 存储到 `etcd` 中，`scheduler` 去找 `apiserver` 拿取 `etcd` 数据的权限，并查看 `etcd` 里面的资源清单并进行资源计算，计算出 **node1 部署 4 个 nginx，node2 部署 6 个 nginx**，`controller` 经过 `apiserver` 找 `scheduler` 拿去资源计算结果，并找 `apiserver` 吩咐 `kubelet` 创建 pod，再经过 `apiserver` 将 pod 交给 `kube-proxy` 配好网络

## K8S 安装

### K8S 安装方式

```bash
1）kubeadm
2）二进制
3）Rancher（高级版，K8S图形化界面）
4）Ansible
5）阿里云ACK、AWS的EKS
```

### 环境准备

| 主机名   | IP         | 角色   | 配置推荐 | 安装软件                                                |
| -------- | ---------- | ------ | -------- | ------------------------------------------------------- |
| master-1 | 10.0.0.110 | master | 1h4g     | APIserver、Controller、scheduler、kubelet、etcd、Docker |
| node-1   | 10.0.0.111 | node1  | 1h2g     | Docker、kubelet、kube-proxy                             |
| node-2   | 10.0.0.112 | node2  | 1h2g     | Docker、kubelet、kube-proxy                             |

### IP 规划

| 三种 Service | IP       |
| ------------ | -------- |
| Pod IP       | 10.2.0.0 |
| Cluster IP   | 10.1.0.0 |
| Node IP      | 10.0.0.0 |

### 安装前环境优化

#### 1）禁用 swap（所有机器）

```bash
# 设置
cat >/etc/sysconfig/kubelet <<EOF
KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
KUBELET_EXTRA_ARGS="--fail-swap-on=false"
EOF
 
sed -i '/swap/d' /etc/fstab 
# 生效
swapoff -a
 
# 检查是否关闭
free -m
```

#### 2）内核参数设置（所有机器）

```bash
# 开启内核防火墙转发
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables=1
net.bridge.bridge-nf-call-iptables=1
net.ipv4.ip_forward=1
EOF
 
# 查看是否设置
sysctl --system
```

#### 3）优化文件描述符（所有机器）

```bash
cat >>/etc/sysctl.conf<<EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
vm.swappiness=0
fs,file-max=52706963
fs.nr_open=52706963
EOF
 
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
 
modprobe overlay
modprobe br_netfilter
```

#### 4）免密登陆（master-1）

```bash
[root@master-1 ~]# ssh-keygen
[root@master-1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub 10.0.0.110
[root@master-1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub 10.0.0.111
[root@master-1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub 10.0.0.112
```

#### 5）host 解析（所有机器）

```bash
# 进行host解析
cat >>/etc/hosts<<EOF
10.0.0.110 master-1 
10.0.0.111 node-1 
10.0.0.112 node-2 
EOF
 
# 查看是否设置成功
for i in master-1 node-1 node-2;do ping -c1 -W1 $i;done
```

#### 6）时间同步（所有机器）

```bash
yum install -y chrony
systemctl start chronyd
systemctl enable chronyd
```

#### 7）加载 IPVS 模块（所有机器）

```bash
# 四层负载
lvs使用的就是IPVS模块
使用的是ipvsadm的命令
 
# 加载IPVS模块
cat  > /etc/sysconfig/modules/ipvs.modules <<EOF
#! /bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod +x /etc/sysconfig/modules/ipvs.modules
source /etc/sysconfig/modules/ipvs.modules
lsmod|grep -e 'ip_vs' -e 'nf_conntrack_ipv'
```

#### 8）安装指定版本 docker（所有机器）

```bash
# 更换官方源
wget -O /etc/yum.repos.d/docker-ce.repo  https://download.docker.com/linux/centos/docker-ce.repo
 
# 安装
yum install -y docker-ce-19.03.15 docker-ce-cli-19.03.15 containerd.io
```

#### 9）配置 docker 镜像加速和 cgroup 驱动（所有机器）

```bash
mkdir -p /etc/docker
tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://7t3bpp45.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
systemctl daemon-reload
systemctl restart docker
```

### 安装 kubeadm

```bash
# 更换官方源（所有机器）
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
 
# 安装指定版本（所有机器）
[root@master-1 ~]# yum install kubelet-1.19.3 kubeadm-1.19.3  kubectl-1.19.3  ipvsadm -y
kubelet：控制node节点启动容器（POD）
kubeadm：帮你安装K8S的类似wegame
kubectl：操作K8S的命令
 
# 启动kubelet
[root@master-1 ~]# systemctl start kubelet
[root@master-1 ~]# systemctl enable kubelet
 
# 初始化集群（master-1）
kubeadm init \
--apiserver-advertise-address=10.0.0.110 \
--image-repository registry.aliyuncs.com/google_containers  \
--kubernetes-version=v1.19.3 \
--service-cidr=10.1.0.0/16 \
--pod-network-cidr=10.2.0.0/16 \
--service-dns-domain=cluster.local \
--ignore-preflight-errors=Swap \
--ignore-preflight-errors=NumCPU
 
Your Kubernetes control-plane has initialized successfully!
 
To start using your cluster, you need to run the following as a regular user:
 
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
 
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/
 
Then you can join any number of worker nodes by running the following on each as root:
 
kubeadm join 10.0.0.110:6443 --token t49hnz.w6naazunbikglmrb \
    --discovery-token-ca-cert-hash sha256:105fb3403018959d10e3accc6c769983c8303909b3bb1f5a547ea309cafc6f76 
 
# 如果初始化时出错（重置初始化，再初始化一遍）（如果已经加入集群时，所有机器都需要执行）
kubeadm reset
 
# 后续操作（master-1）
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
 
# 查看集群节点
[root@master-1 ~]# kubectl get node
NAME       STATUS     ROLES    AGE    VERSION
master-1   NotReady   master   2m7s   v1.19.3
 
# 修改网络模式为ipvs
[root@master-1 ~]# kubectl edit cm kube-proxy -n kube-system
修改 mode:"" 为 mode:"ipvs"
 
# 重启服务
[root@master-1 ~]# kubectl -n kube-system get pod|grep kube-proxy|awk '{print "kubectl -n kube-system delete pod "$1}'|bash
 
# 查看kube-proxy
[root@master-1 ~]# kubectl get -n kube-system pod|grep 'kube-proxy'
kube-proxy-lslj6                   1/1     Running   0          38s
```

### node 上加入 master 集群（node-1，node-2）

```bash
# 将node加入master
systemctl restart kubelet docker
systemctl enable kubelet docker
kubeadm join 10.0.0.110:6443 --token t49hnz.w6naazunbikglmrb \
    --discovery-token-ca-cert-hash sha256:105fb3403018959d10e3accc6c769983c8303909b3bb1f5a547ea309cafc6f76 
 
# 查看集群状态
[root@master-1 ~]# kubectl get node
NAME       STATUS     ROLES    AGE   VERSION
master-1   NotReady   master   13m   v1.19.3
node-1     NotReady   <none>   16s   v1.19.30
node-2     NotReady   <none>   9s    v1.19.3
 
# 查看kube-proxy
[root@master-1 ~]# kubectl get -n kube-system pod|grep 'kube-proxy'
kube-proxy-lslj6                   1/1     Running   0          38s
kube-proxy-s4d4z                   1/1     Running   0          26s
kube-proxy-tvhlb                   1/1     Running
0          32s
 
# 如果6443端口不存在（apiserver没启动）
systemctl start kubelet
```

## 配置 flannel

```bash
# 下载flannel代码
进入网站下载：https://github.com/flannel-io/flannel/blob/master/Documentation/kube-flannel.yml
 
# 修改资源清单
kube-flannel.yml
Network:"10.2.0.0/16"
- --iface=eth0
       containers:
       - name: kube-flannel
         image: quay.io/coreos/flannel:v0.22.3
         command:
         - /opt/bin/flanneld
         args:
         - --ip-masq
         - --kube-subnet-mgr
         - --iface=eth0
 
# 下载镜像（所有机器）
docker pull docker.io/flannel/flannel:v0.22.3
docker pull docker.io/flannel/flannel-cni-plugin:v1.2.0
 
# 执行资源清单
[root@master-1 ~]# kubectl apply -f kube-flannel.yml 
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
 
# 查看flannel是否启动成功
[root@master-1 ~]# kubectl get pod -n kube-flannel 
NAME                    READY   STATUS    RESTARTS   AGE
kube-flannel-ds-krcl4   1/1     Running   0          93s
kube-flannel-ds-wfqbx   1/1     Running   0          93s
kube-flannel-ds-xflzc   1/1     Running   0          93s
 
# 查看集群状态
[root@master-1 ~]# kubectl get nodes 
NAME       STATUS   ROLES    AGE   VERSION
master-1   Ready    master   21h   v1.19.3
node-1     Ready    <none>   20h   v1.19.3
node-2     Ready    <none>   20h   v1.19.3
```

## 给角色打标签

```bash
# 添加标签
[root@master-1 ~]# kubectl label nodes node-1 node-role.kubernetes.io/node01=
node/node-1 labeled
 
# 删除标签
[root@master-1 ~]# kubectl label nodes node-1 node-role.kubernetes.io/node-
node/node-1 labeled
 
# 解读
kubectl label nodes # 打标签
node-1 # 节点机
node-role.kubernetes.io/ # api端口
node # 标签名
- # 删除
= # 添加
 
# 查看集群标签
[root@master-1 ~]# kubectl get nodes 
NAME       STATUS   ROLES    AGE   VERSION
master-1   Ready    master   21h   v1.19.3
node-1     Ready    node01   21h   v1.19.3
node-2     Ready    node02   21h   v1.19.3
```

## 小秘密

```bash
[root@master-1 ~]# cat ~/.bashrc
# .bashrc
 
# User specific aliases and functions
 
alias rm='rm -i'
alias cp='cp -i'
alias mv='mv -i'
 
# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
export http_proxy=http://127.0.0.1:7890
 
[root@master-1 ~]# source ~/.bashrc
 
# 解除
[root@master-1 ~]# unset http_proxy
```

## K8S 命令补全

```bash
yum install -y bash-completion
source /usr/share/bash-completion/bash_completion 
source <(kubectl completion bash) 
kubectl completion bash > /etc/bash_completion.d/kubectl
```

## 图形化界面

```bash
https://www.rancher.com/quick-start   
```

## K8S 图形化界面

```bash
# 下载资源
k8s_dashboard.tgz
 
# 解压资源
[root@master-1 ~]# tar xf k8s_dashboard.tgz 
 
# 推送资源到node节点上
[root@master-1 ~]# for i in node-1 node-2;do scp dashboard.v2.0.0.tar.gz metrics-scraper.v1.0.4.tar.gz $i:/root;done
 
# 导入docker镜像
[root@node-1 ~]# docker load < dashboard.v2.0.0.tar.gz 
[root@node-1 ~]# docker load < metrics-scraper.v1.0.4.tar.gz 
[root@node-2 ~]# docker load < dashboard.v2.0.0.tar.gz 
[root@node-2 ~]# docker load < metrics-scraper.v1.0.4.tar.gz
 
# 执行资源清单
[root@master-1 ~]# kubectl apply -f recommended-2.0.yaml 
 
# 创建用户授权
[root@master-1 ~]# kubectl create serviceaccount  dashboard-admin -n kubernetes-dashboard
[root@master-1 ~]# kubectl create clusterrolebinding  \
dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin
 
# 获取token
kubectl describe secrets \
-n kubernetes-dashboard $(kubectl -n kubernetes-dashboard get secret | awk '/dashboard-admin/{print $1}')
 
## 如果有权限修改或者配置修改
# 重启资源清单
[root@master-1 ~]# kubectl delete -f recommended-2.0.yaml
[root@master-1 ~]# kubectl apply -f recommended-2.0.yaml 
# 删除存在的用户
[root@master-1 ~]# kubectl delete clusterrolebinding dashboard-admin
# 创建用户授权
[root@master-1 ~]# kubectl create serviceaccount  dashboard-admin -n kubernetes-dashboard
[root@master-1 ~]# kubectl create clusterrolebinding  \
dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin
# 获取token
kubectl describe secrets \
-n kubernetes-dashboard $(kubectl -n kubernetes-dashboard get secret | awk '/dashboard-admin/{print $1}')
```



![image-20230919104405297](https://www.xiaoyuanwiki.com/image/image-20230919104405297.png)





![image-20230919105842829](https://www.xiaoyuanwiki.com/image/image-20230919105842829.png)



**思考问题：为什么 pod 启再 node1 上但是用 master 访问的了？**

**答：因为 kube-proxy 的原因所有机器都进行了端口映射**